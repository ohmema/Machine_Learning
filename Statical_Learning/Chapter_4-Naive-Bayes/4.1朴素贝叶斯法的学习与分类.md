# 朴素贝叶斯的基本方法

* 输入空间 X  n维向量的集合
* 输出空间 Y 类标记集合{c1,c2...ck}

朴素贝叶斯学习：先验概率分布+条件概率分布-----> 联合概率分布P(X,Y)


* 先验概率分布 P(Y=ck),k=1,···K

* 条件概率分布  P(X=x|Y=ck) = P(X1=x1,...Xn=xn|Y=ck),k=1,2,3...K

## 朴素贝叶斯----条件独立性假设

> 用于分类的特征在类确定的条件下都是条件独立的

![](https://github.com/LiuChuang0059/Machine_Learning/blob/master/Statical_Learning/Chapter_4-Naive-Bayes/picture/%E8%B4%9D%E5%8F%B6%E6%96%AF-%E6%9D%A1%E4%BB%B6%E7%8B%AC%E7%AB%8B.png)


## 朴素贝叶斯分类器
* 对于给定的输入x，通过学习的模型计算后验概率分布
* 后验概率最大的类作为x的类输出。

**后验概率最大化-----> 期望风险最小化**
![](https://github.com/LiuChuang0059/Machine_Learning/blob/master/Statical_Learning/Chapter_4-Naive-Bayes/picture/%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%86%E7%B1%BB%E5%99%A8.png)

