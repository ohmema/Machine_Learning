# 1.监督学习

* 回归
* 分类


# 2.无监督学习


* 无监督学习中没有任何的标签或者是有相同的标签或者就是没标签

* 无监督学习算法可能会把这些数据分成两个不同的簇。所以叫做聚类算法


![](https://github.com/fengdu78/Coursera-ML-AndrewNg-Notes/blob/master/images/94f0b1d26de3923fc4ae934ec05c66ab.png)


# 单边量线性回归

## 1.模型

## 2.cost-function

![](https://github.com/fengdu78/Coursera-ML-AndrewNg-Notes/blob/master/images/10ba90df2ada721cf1850ab668204dc9.png)



#### 图像理解 
![](https://github.com/fengdu78/Coursera-ML-AndrewNg-Notes/blob/master/images/0b789788fc15889fe33fb44818c40852.png)


* 找到合适的参数，使得最小，即达到中心点
![](https://github.com/fengdu78/Coursera-ML-AndrewNg-Notes/blob/master/images/86c827fe0978ebdd608505cd45feb774.png)

## 梯度下降

* 直观表现---起始点不同，得到的最低点不一样

![]()

* 参数同时更新


![]()


* 学习效率选取过大，导致结果发散

！[]()


* 学习效率固定，下降速度也在逐渐减小


![]()



## 梯度下降的线性回归


* 求解代价函数导数

![]()
* 注意同时更新
![]()

* 批量梯度下降

> 在梯度下降的每一步中，我们都用到了所有的训练样本，在梯度下降中，在计算微分求导项时，我们需要进行求和运算，所以，在每一个单独的梯度下降中，我们最终都要计算这样一个东西，这个项需要对所有m个训练样本求和












# 参考

* https://github.com/fengdu78/Coursera-ML-AndrewNg-Notes/blob/master/markdown/week1.md
* https://blog.csdn.net/abcjennifer/article/details/7691571
